{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terrain = \\\n",
    "# '''\n",
    "#  \\                                              /\n",
    "#   \\      /wwwww\\                               /\n",
    "#    \\    /       \\            _____            /\n",
    "#     \\__/         \\          /     \\          /\n",
    "#                   \\        /       \\        /\n",
    "#                    \\      /         \\      /\n",
    "#                     \\    /           \\    /\n",
    "#                      \\__/             \\__/\n",
    "# '''\n",
    "terrain = \\\n",
    "'''\n",
    "\\             /\n",
    " \\           /\n",
    "  \\__/www\\__/\n",
    "'''\n",
    "chars = ['\\\\', '-', '_', '/', 'w']\n",
    "grid = terrain.split('\\n')[1:-1]\n",
    "max_length = max([len(r) for r in grid])\n",
    "states = []\n",
    "for a in range(max_length):\n",
    "    for b in range(len(grid)):\n",
    "        if grid[b][a] != ' ':\n",
    "            states.append(grid[b][a])\n",
    "            break\n",
    "\n",
    "# AVERAGING_RATE = .1\n",
    "DISCOUNT = .9\n",
    "LEARNING_RATE = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_momentum_dict = {\n",
    "    'right': 1,\n",
    "    'left': -1,\n",
    "    'neutral': 0\n",
    "}\n",
    "state_momentum_dict = {\n",
    "    '\\\\': 1,\n",
    "    '/': -1,\n",
    "    '_': 0,\n",
    "    'w': 0,\n",
    "    'x': 0\n",
    "}\n",
    "\n",
    "# Q = Weights * Feature Vector\n",
    "def phi(states, state, momentum, action_digit):\n",
    "\n",
    "    phi_vector = []\n",
    "\n",
    "    # Momentum\n",
    "    phi_vector.append(momentum/50)\n",
    "\n",
    "    # Direction (One Hot Encoding) (OHE)\n",
    "    if momentum > 0:\n",
    "        phi_vector.extend([0, 0, 1])\n",
    "    elif momentum < 0:\n",
    "        phi_vector.extend([1, 0, 0])\n",
    "    else:\n",
    "        phi_vector.extend([0, 1, 0])\n",
    "\n",
    "    # Action (OHE)\n",
    "    if action_digit == 1:\n",
    "        phi_vector.extend([0, 0, 1])\n",
    "    elif action_digit == -1:\n",
    "        phi_vector.extend([1, 0, 0])\n",
    "    else:\n",
    "        phi_vector.extend([0, 1, 0])\n",
    "\n",
    "    w_states = []\n",
    "    for i, s in enumerate(states):\n",
    "        if s == 'w':\n",
    "            w_states.append(i)\n",
    "    num_positive_slopes = 0\n",
    "    num_negative_slopes = 0\n",
    "    num_neutral_slopes = 0\n",
    "    if state in w_states:\n",
    "        phi_vector.append(0) # Distance\n",
    "        phi_vector.extend([0, 0, 0]) # Slope Types\n",
    "        pass\n",
    "    else:\n",
    "        if state > w_states[-1]:\n",
    "            phi_vector.append((state - w_states[-1])/50) # Distance\n",
    "            for i in range(w_states[-1], state+1):\n",
    "                if states[i] == '\\\\':\n",
    "                    num_positive_slopes += 1\n",
    "                elif states[i] == '/':\n",
    "                    num_negative_slopes += 1\n",
    "                else:\n",
    "                    num_neutral_slopes += 1\n",
    "        else:\n",
    "            phi_vector.append((w_states[0] - state)/50) # Distance\n",
    "            for i in range(state, w_states[0]+1):\n",
    "                if states[i] == '\\\\':\n",
    "                    num_positive_slopes += 1\n",
    "                elif states[i] == '/':\n",
    "                    num_negative_slopes += 1\n",
    "                else:\n",
    "                    num_neutral_slopes += 1\n",
    "        total_slopes = num_positive_slopes + num_negative_slopes + num_neutral_slopes\n",
    "        phi_vector.extend([num_positive_slopes/total_slopes, num_negative_slopes/total_slopes, num_neutral_slopes/total_slopes]) # Slope Types\n",
    "\n",
    "    tile_encoding = [0] * 20\n",
    "    tile_encoding[round((state/len(states)) * (len(tile_encoding) - 1))] = (num_positive_slopes - num_negative_slopes)/50\n",
    "    phi_vector.extend(tile_encoding) # Tile Encoding, Distance to Goal\n",
    "\n",
    "    return np.array(phi_vector)\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(q_weights, states, state, momentum, epsilon=0.1):\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(['right', 'left', 'neutral'])\n",
    "    else:\n",
    "        best_q = float('-inf')\n",
    "        best_action = []\n",
    "        for a in ['right', 'left', 'neutral']:\n",
    "            q_val = np.dot(q_weights, phi(states, state, momentum, action_momentum_dict[a]))\n",
    "            if best_q < q_val:\n",
    "                best_q = q_val\n",
    "                best_action = [a]\n",
    "            elif best_q == q_val:\n",
    "                best_action.append(a)\n",
    "\n",
    "        return random.choice(best_action)\n",
    "\n",
    "\n",
    "def semi_gradient_SARSA(states, lr=LEARNING_RATE, d=DISCOUNT, episode_count=500):\n",
    "\n",
    "    weights_len = len(phi(states, 0, 0, 0))\n",
    "    q_weights = np.random.uniform(-1, 1, size=weights_len)\n",
    "\n",
    "    starting_states = []\n",
    "    for i, s in enumerate(states):\n",
    "        if s != 'w' and s != 'x':\n",
    "            starting_states.append(i)\n",
    "\n",
    "    while episode_count > 0:\n",
    "        episode_count -= 1\n",
    "        \n",
    "        # Initialize S\n",
    "        s = random.choice(starting_states)\n",
    "        m = 0\n",
    "        a = epsilon_greedy_policy(q_weights, states, s, m)\n",
    "        r = 0\n",
    "        avg_r_est = 0 # Not a true average reward, only an est. at best\n",
    "        fin_ep = False\n",
    "\n",
    "        while not fin_ep:\n",
    "            # Take action A, observe R, S'\n",
    "            m_prime = m + action_momentum_dict[a] + state_momentum_dict[states[s]]\n",
    "            \n",
    "            if m_prime > 0:\n",
    "                s_prime = min(len(states)-1, s+1)\n",
    "            elif m_prime < 0:\n",
    "                s_prime = max(0, s-1)\n",
    "            else:\n",
    "                s_prime = s\n",
    "\n",
    "            m_prime += state_momentum_dict[states[s_prime]]\n",
    "\n",
    "            if states[s_prime] == 'w':\n",
    "                if m_prime == 0:\n",
    "                    r = 10\n",
    "                    fin_ep = True\n",
    "                else:\n",
    "                    r = -1\n",
    "            elif s == s_prime:\n",
    "                r = -5\n",
    "            else:\n",
    "                r = -1\n",
    "\n",
    "                \n",
    "            # Choose A' as a function q(S', ., w) using epsilon greedy\n",
    "            a_prime = epsilon_greedy_policy(q_weights, states, s, m)\n",
    "            \n",
    "            delta = r - avg_r_est \\\n",
    "                + np.dot(q_weights, phi(states, s_prime, m_prime, action_momentum_dict[a_prime])) \\\n",
    "                - np.dot(q_weights, phi(states, s, m, action_momentum_dict[a]))\n",
    "            avg_r_est += b * delta\n",
    "            q_weights += lr * delta * phi(states, s, m, action_momentum_dict[a])\n",
    "\n",
    "            # Update S, A with S', A' accordingly\n",
    "            s = s_prime\n",
    "            m = m_prime\n",
    "            a = a_prime\n",
    "    \n",
    "    return q_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_weights = semi_gradient_SARSA(states, episode_count=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.95405285e-01, -2.80669307e+02, -2.76743069e+02, -2.81232547e+02,\n",
       "       -2.79721964e+02, -2.79704218e+02, -2.79552507e+02])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: right, Q-value: -556.2955764094316\n",
      "Action: left, Q-value: -556.4650333931065\n",
      "Action: neutral, Q-value: -556.4472874821809\n"
     ]
    }
   ],
   "source": [
    "for a in ['right', 'left', 'neutral']:\n",
    "    q_val = np.dot(q_weights, phi(states, 0, 0, action_momentum_dict[a]))\n",
    "    print(f\"Action: {a}, Q-value: {q_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(q_weights, states, state, momentum):\n",
    "\n",
    "    return epsilon_greedy_policy(q_weights, states, state, momentum)\n",
    "\n",
    "def test_weights(states, starting_state, q_weights):\n",
    "        \n",
    "    # Initialize S\n",
    "    s = starting_state\n",
    "    m = 0\n",
    "    action_history = []\n",
    "    a = greedy_policy(q_weights, states, s, m)\n",
    "    \n",
    "    while True:\n",
    "        print(a)\n",
    "\n",
    "        action_history.append(a)\n",
    "\n",
    "        # Take action A, observe R, S'\n",
    "        m_prime = m + action_momentum_dict[a] + state_momentum_dict[states[s]]\n",
    "        \n",
    "        if m_prime > 0:\n",
    "            s_prime = min(len(states)-1, s+1)\n",
    "        elif m_prime < 0:\n",
    "            s_prime = max(0, s-1)\n",
    "        else:\n",
    "            s_prime = s\n",
    "\n",
    "        m_prime += state_momentum_dict[states[s_prime]]\n",
    "\n",
    "        if states[s_prime] == 'w':\n",
    "            if m_prime == 0:\n",
    "                break\n",
    "            \n",
    "        # Choose A' as a function q(S', ., w) using greedy\n",
    "        a_prime = greedy_policy(q_weights, states, s, m)\n",
    "                \n",
    "        # Update S, A with S', A' accordingly\n",
    "        s = s_prime\n",
    "        m = m_prime\n",
    "        a = a_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\             /\n",
      " \\           /\n",
      "  \\__/www\\__/\n",
      "\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "neutral\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "left\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "neutral\n",
      "right\n",
      "right\n",
      "neutral\n",
      "right\n",
      "right\n",
      "neutral\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n",
      "neutral\n",
      "right\n",
      "left\n",
      "right\n",
      "right\n",
      "right\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "print(terrain)\n",
    "test_weights(states, 0, q_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentzerQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
