{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = .9\n",
    "LEARNING_RATE = .1\n",
    "CONVERGENCE_THRESHOLD = .01\n",
    "\n",
    "\n",
    "def greedy_policy(q, player_X, player_Y):\n",
    "\n",
    "    max_q = max(q[player_Y][player_X].values())\n",
    "    max_actions = [action for action, value in q[player_Y][player_X].items() if value == max_q]\n",
    "    return random.choice(max_actions)\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(q, player_X, player_Y, epsilon=0.3\n",
    "):\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(['N', 'E', 'S', 'W'])\n",
    "    else:\n",
    "        max_q = max(q[player_Y][player_X].values())\n",
    "        max_actions = [action for action, value in q[player_Y][player_X].items() if value == max_q]\n",
    "        return random.choice(max_actions)\n",
    "    \n",
    "\n",
    "# None, None: User fell off board\n",
    "def take_a(windyworld, wind, a, player_X, player_Y):\n",
    "\n",
    "    direction_map = {\n",
    "        'N': (0, -1),  # Move North (decrease Y)\n",
    "        'S': (0, 1),   # Move South (increase Y)\n",
    "        'E': (1, 0),   # Move East (increase X)\n",
    "        'W': (-1, 0),  # Move West (decrease X)\n",
    "    }\n",
    "\n",
    "    delta_X, delta_Y = direction_map[a]\n",
    "    new_X = player_X + delta_X\n",
    "    new_Y = player_Y + delta_Y\n",
    "    \n",
    "    # Check if the initial movement is within bounds\n",
    "    if 0 <= new_X and new_X < len(windyworld[0]) and 0 <= new_Y and new_Y < len(windyworld):\n",
    "\n",
    "        if windyworld[new_Y][new_X] == 'X':\n",
    "            return None, None\n",
    "        \n",
    "        wind_map = {\n",
    "            '': 0,   # No wind\n",
    "            '^': -1, # Upward wind\n",
    "            'v': 1   # Downward wind\n",
    "        }\n",
    "        new_Y_wind = new_Y + wind_map[wind[new_X]]\n",
    "        # Check if movement w/ wind is within bounds\n",
    "        if 0 <= new_Y_wind and new_Y_wind < len(windyworld):\n",
    "\n",
    "            if windyworld[new_Y_wind][new_X] == 'X':\n",
    "                return None, None\n",
    "            else:\n",
    "                return new_X, new_Y_wind\n",
    "        \n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(windyworld, wind, lr=LEARNING_RATE, d=DISCOUNT, ct=CONVERGENCE_THRESHOLD, min_episode_count=1000):\n",
    "\n",
    "    q = [[{'N': 0, 'E': 0, 'S': 0, 'W': 0} for _ in r] for r in windyworld]\n",
    "\n",
    "    starting_states = []\n",
    "    goal_X = 0\n",
    "    goal_Y = 0\n",
    "    for y in range(len(windyworld)):\n",
    "        for x in range(len(windyworld[y])):\n",
    "            if windyworld[y][x] == '_':\n",
    "                starting_states.append((x, y))\n",
    "            if windyworld[y][x] == '1':\n",
    "                goal_X = x\n",
    "                goal_Y = y\n",
    "\n",
    "    episode_count = 0\n",
    "    while True:\n",
    "        episode_count += 1\n",
    "        player_X1, player_Y1 = random.choice(starting_states) # S\n",
    "        greedy_eps_A1 = epsilon_greedy_policy(q, player_X, player_Y) # A\n",
    "        td_sum = 0\n",
    "        while True:\n",
    "\n",
    "            # Take action A and observe R and S'\n",
    "            upd_X, upd_Y = take_a(windyworld, wind, greedy_eps_A1, player_X, player_Y) # S'\n",
    "            terminal = False\n",
    "            r = 0 # R\n",
    "            if upd_X == None and upd_Y == None:\n",
    "                terminal = True\n",
    "                r = -10\n",
    "            elif upd_X == player_X and upd_Y == player_Y:\n",
    "                r = -1\n",
    "            elif windyworld[upd_Y][upd_X] == '1':\n",
    "                terminal = True\n",
    "                r = 10\n",
    "            else: # windyworld[upd_Y][upd_X] == '_':\n",
    "                current_dist = abs(player_X - goal_X) + abs(player_Y - goal_Y)\n",
    "                new_dist = abs(upd_X - goal_X) + abs(upd_Y - goal_Y)\n",
    "                if new_dist < current_dist:\n",
    "                    r = 0\n",
    "                else:\n",
    "                    r = -1\n",
    "\n",
    "            greedy_eps_A2 = epsilon_greedy_policy(q, upd_X, upd_Y) # A'\n",
    "            \n",
    "            if terminal: # S' is terminal, therefore, max_a Q(S', a) = 0\n",
    "                # Q(S, A) <- Q(S, A) + lr[R - Q(S, A)]\n",
    "                td = (lr * (r - q[player_Y][player_X][greedy_eps_A1]))\n",
    "                td_sum += td\n",
    "                q[player_Y][player_X][greedy_eps_A1] += td\n",
    "                break\n",
    "            else:\n",
    "                # Q(S, A) <- Q(S, A) + lr[R + d*Q(S', A') - Q(S, A)]\n",
    "                td = (lr * (r + (d * q[upd_Y][upd_X][greedy_eps_A2])  - q[player_Y][player_X][greedy_eps_A1]))\n",
    "                td_sum += td\n",
    "                q[player_Y][player_X][greedy_eps_A1] += td\n",
    "                \n",
    "                # S' -> S\n",
    "                player_X = upd_X\n",
    "                player_Y = upd_Y\n",
    "                # A' -> A\n",
    "                greedy_eps_A2 = greedy_eps_A1\n",
    "\n",
    "        if episode_count >= min_episode_count and td_sum <= ct:\n",
    "            break\n",
    "\n",
    "    return q\n",
    "\n",
    "'''\n",
    "Q_learning is not SARSA with greedy, there are a few key differences:\n",
    "\n",
    "SARSA evaluates S based on S' (given A) and A' selected from epsilon-greedy and goes along that path.\n",
    "Updates for Q values are based on current (and possibly non optimal) state path.\n",
    "\n",
    "Q_learning evaluates S based on S' (given A) and A' selected from greedy, but may not follow that exact path.\n",
    "Updates for Q values are based on best (and optimal, but possibly not current) state path. \n",
    "'''\n",
    "def Q_learning(windyworld, wind, lr=LEARNING_RATE, d=DISCOUNT, ct=CONVERGENCE_THRESHOLD, min_episode_count=1000):\n",
    "\n",
    "    q = [[{'N': 0, 'E': 0, 'S': 0, 'W': 0} for _ in r] for r in windyworld]\n",
    "\n",
    "    starting_states = []\n",
    "    goal_X = 0\n",
    "    goal_Y = 0\n",
    "    for y in range(len(windyworld)):\n",
    "        for x in range(len(windyworld[y])):\n",
    "            if windyworld[y][x] == '_':\n",
    "                starting_states.append((x, y))\n",
    "            if windyworld[y][x] == '1':\n",
    "                goal_X = x\n",
    "                goal_Y = y\n",
    "\n",
    "    episode_count = 0\n",
    "    while True:\n",
    "        episode_count += 1\n",
    "        player_X, player_Y = random.choice(starting_states) # S\n",
    "        td_sum = 0\n",
    "        while True:\n",
    "            greedy_eps_A = epsilon_greedy_policy(q, player_X, player_Y) # A\n",
    "            upd_X, upd_Y = take_a(windyworld, wind, greedy_eps_A, player_X, player_Y) # S'\n",
    "            \n",
    "            terminal = False\n",
    "            r = 0 # R\n",
    "            if upd_X == None and upd_Y == None:\n",
    "                terminal = True\n",
    "                r = -10\n",
    "            elif upd_X == player_X and upd_Y == player_Y:\n",
    "                r = -1\n",
    "            elif windyworld[upd_Y][upd_X] == '1':\n",
    "                terminal = True\n",
    "                r = 10\n",
    "            else: # windyworld[upd_Y][upd_X] == '_':\n",
    "                current_dist = abs(player_X - goal_X) + abs(player_Y - goal_Y)\n",
    "                new_dist = abs(upd_X - goal_X) + abs(upd_Y - goal_Y)\n",
    "                if new_dist < current_dist:\n",
    "                    r = 0\n",
    "                else:\n",
    "                    r = -1\n",
    "\n",
    "            if terminal: # S' is terminal, therefore, max_a Q(S', a) = 0\n",
    "                # Q(S, A) <- Q(S, A) + lr[R - Q(S, A)]\n",
    "                td = (lr * (r - q[player_Y][player_X][greedy_eps_A]))\n",
    "                td_sum += td\n",
    "                q[player_Y][player_X][greedy_eps_A] += td\n",
    "                break\n",
    "            else:\n",
    "                # Q(S, A) <- Q(S, A) + lr[R + d*max_a(Q(S', a)) - Q(S, A)]\n",
    "                best_a = greedy_policy(q, upd_X, upd_Y)\n",
    "                td = (lr * (r + (d * q[upd_Y][upd_X][best_a])  - q[player_Y][player_X][greedy_eps_A]))\n",
    "                td_sum += td\n",
    "                q[player_Y][player_X][greedy_eps_A] += td\n",
    "                \n",
    "                # S' -> S\n",
    "                player_X = upd_X\n",
    "                player_Y = upd_Y\n",
    "\n",
    "        if episode_count >= min_episode_count and td_sum <= ct:\n",
    "            break\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v\tv\t>\tv\tv\tX\tv\t<\t\n",
      ">\t>\t>\t>\tv\tX\t1\t<\t\n",
      ">\t^\t>\t>\tv\tX\t^\t<\t\n",
      ">\t^\tX\t>\tv\tX\t^\t<\t\n",
      ">\t^\tX\t>\tv\tX\t^\t<\t\n",
      ">\t^\tX\t>\tv\t>\t^\t<\t\n",
      ">\t^\tX\t>\t>\t>\t^\t<\t\n",
      ">\t^\tX\t^\t^\t^\t^\t<\t\n",
      "\n",
      "v\t\tv\t\t\t^\t\tv\n"
     ]
    }
   ],
   "source": [
    "windyworld = [\n",
    "    ['_', '_', '_', '_', '_', 'X', '_', '_'],\n",
    "    ['_', '_', '_', '_', '_', 'X', '1', '_'],\n",
    "    ['_', '_', '_', '_', '_', 'X', '_', '_'],\n",
    "    ['_', '_', 'X', '_', '_', 'X', '_', '_'],\n",
    "    ['_', '_', 'X', '_', '_', 'X', '_', '_'],\n",
    "    ['_', '_', 'X', '_', '_', '_', '_', '_'],\n",
    "    ['_', '_', 'X', '_', '_', '_', '_', '_'],\n",
    "    ['_', '_', 'X', '_', '_', '_', '_', '_']\n",
    "]\n",
    "wind = ['v', '', 'v', '', '', '^', '', 'v']\n",
    "\n",
    "q = Q_learning(windyworld, wind)\n",
    "\n",
    "replace_dict = {\n",
    "    'N': '^',\n",
    "    'E': '>',\n",
    "    'S': 'v',\n",
    "    'W': '<'\n",
    "}\n",
    "\n",
    "for y in range(len(windyworld)):\n",
    "    line = ''\n",
    "    for x in range(len(windyworld[y])):\n",
    "        if windyworld[y][x] != '_':\n",
    "            line += windyworld[y][x]\n",
    "        else:\n",
    "            max_q = max(q[y][x].values())\n",
    "            max_actions = [action for action, value in q[y][x].items() if value == max_q]\n",
    "            line += ''.join(max_actions)\n",
    "        line += '\\t'\n",
    "        for key in list(replace_dict):\n",
    "            line = line.replace(key, replace_dict[key])\n",
    "    print(line)\n",
    "print()\n",
    "print('\\t'.join(wind))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentzerQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
