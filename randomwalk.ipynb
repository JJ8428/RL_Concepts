{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a variant of random walk. The agent can walk to any adjacent state, or the agent can teleport to a random state in hopes of getting closer to state 0.\n",
    "'''\n",
    "\n",
    "states = [\n",
    "    {'terminal': True, 'reward': 25}, # 0\n",
    "    {'terminal': False, 'reward': 0}, # 1\n",
    "    {'terminal': False, 'reward': 0}, # 2\n",
    "    {'terminal': False, 'reward': -100}, # 3\n",
    "    {'terminal': False, 'reward': 0}, # 4\n",
    "    {'terminal': False, 'reward': -100}, # 5\n",
    "    {'terminal': False, 'reward': 0}, # 6\n",
    "    {'terminal': False, 'reward': 0}, # 7\n",
    "    {'terminal': True, 'reward': 25} # 8\n",
    "]\n",
    "\n",
    "DISCOUNT = .9\n",
    "LEARNING_RATE = .1\n",
    "CONVERGENCE_THRESHOLD = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(q, state):\n",
    "\n",
    "    max_q = max(q[state].values())\n",
    "    max_actions = [action for action, value in q[state].items() if value == max_q]\n",
    "    return random.choice(max_actions)\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(q, state, epsilon=0.3):\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        # return random.choice(['left', 'right', 'teleport'])\n",
    "        return random.choice(['left', 'right'])\n",
    "    else:\n",
    "        max_q = max(q[state].values())\n",
    "        max_actions = [action for action, value in q[state].items() if value == max_q]\n",
    "        return random.choice(max_actions)\n",
    "\n",
    "\n",
    "def n_step_SARSA(states, n=2, lr=LEARNING_RATE, d=DISCOUNT, ct=CONVERGENCE_THRESHOLD, min_episode_count=10000):\n",
    "\n",
    "    q = {}\n",
    "    starting_states = []\n",
    "    for i, s in enumerate(states):\n",
    "        if not s['terminal']:\n",
    "            q[i] = {'left': 0, 'right': 0, 'teleport': 0}\n",
    "            # q[i] = {'left': 0, 'right': 0}\n",
    "            starting_states.append(i)\n",
    "\n",
    "    episode_count = 0\n",
    "    while True:\n",
    "        episode_count += 1\n",
    "        episode = {\n",
    "            's': {},\n",
    "            'a': {},\n",
    "            'r': {},\n",
    "        }\n",
    "        t = 0\n",
    "        T = float('inf')\n",
    "\n",
    "        # Select and store s_0, a_0\n",
    "        state = random.choice(starting_states)\n",
    "        action = epsilon_greedy_policy(q, state)\n",
    "        episode['a'][t] = action\n",
    "        episode['s'][t] = state\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                # Take a_t, observe and store s_t+1 and r_t+1\n",
    "                match action:\n",
    "                    case 'left':\n",
    "                        state -= 1\n",
    "                    case 'right':\n",
    "                        state += 1\n",
    "                    case 'teleport':\n",
    "                        state = random.randint(0, len(states)-1)\n",
    "                episode['s'][t+1] = state\n",
    "                reward = states[state]['reward']\n",
    "                episode['r'][t+1] = reward\n",
    "\n",
    "                if states[state]['terminal']: \n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Select and store a_t+1\n",
    "                    action = epsilon_greedy_policy(q, state)\n",
    "                    episode['a'][t+1] = action\n",
    "\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                G = 0\n",
    "                for i in range(tau + 1, min(tau + n, T)):\n",
    "                    G += (d ** (i - tau - 1)) * episode['r'][i]\n",
    "                if tau + n < T:\n",
    "                    G += (d ** n) * q[episode['s'][tau + n]][episode['a'][tau + n]]\n",
    "\n",
    "                q[episode['s'][tau]][episode['a'][tau]] += lr * (G - q[episode['s'][tau]][episode['a'][tau]])\n",
    "\n",
    "            t += 1\n",
    "            \n",
    "            if tau == T - 1:\n",
    "                break\n",
    "        if episode_count >= min_episode_count:\n",
    "            break\n",
    "   \n",
    "    return q\n",
    "\n",
    "q = n_step_SARSA(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t2\t3\t4\t5\t6\t7\t8\t\n",
      "25\t0\t0\t-100\t0\t-100\t0\t0\t25\t\n",
      "*\t<-\t<-\t<-\tT\t->\t->\t->\t*\t\n"
     ]
    }
   ],
   "source": [
    "line1 = ''\n",
    "line2 = ''\n",
    "line3 = ''\n",
    "for a in range(0, len(states)):\n",
    "    line1 += str(a) + '\\t'\n",
    "    line2 += str(states[a]['reward']) + '\\t'\n",
    "    if states[a]['terminal']:\n",
    "        line3 += '*'\n",
    "    else:\n",
    "        max_a = max(q[a], key=q[a].get)\n",
    "        line3 += max_a\n",
    "    line3 += '\\t'  \n",
    "print(line1)\n",
    "print(line2)\n",
    "print(line3.replace('teleport', 'T').replace('left', '<-').replace('right', '->'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_actions(action_dict, temperature=.1):\n",
    "\n",
    "    q_values = np.array(list(action_dict.values()))\n",
    "    scaled_q = q_values / temperature\n",
    "    exp_q = np.exp(scaled_q - np.max(scaled_q))\n",
    "    prob = exp_q / np.sum(exp_q)\n",
    "    return {'left': prob[0], 'right': prob[1], 'teleport': prob[2]}\n",
    "\n",
    "\n",
    "def n_step_Tree_Backup(states, n=2, lr=LEARNING_RATE, d=DISCOUNT, ct=CONVERGENCE_THRESHOLD, min_episode_count=10000):\n",
    "\n",
    "    q = {}\n",
    "    starting_states = []\n",
    "    for i, s in enumerate(states):\n",
    "        if not s['terminal']:\n",
    "            q[i] = {'left': 0, 'right': 0, 'teleport': 0}\n",
    "            # q[i] = {'left': 0, 'right': 0}\n",
    "            starting_states.append(i)\n",
    "\n",
    "    episode_count = 0\n",
    "    while True:\n",
    "        episode_count += 1\n",
    "        episode = {\n",
    "            's': {},\n",
    "            'a': {},\n",
    "            'r': {},\n",
    "        }\n",
    "        t = 0\n",
    "        T = float('inf')\n",
    "\n",
    "        # Select and store s_0, a_0\n",
    "        state = random.choice(starting_states)\n",
    "        action = epsilon_greedy_policy(q, state)\n",
    "        episode['a'][t] = action\n",
    "        episode['s'][t] = state\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                # Take a_t, observe and store s_t+1 and r_t+1\n",
    "                match action:\n",
    "                    case 'left':\n",
    "                        state -= 1\n",
    "                    case 'right':\n",
    "                        state += 1\n",
    "                    case 'teleport':\n",
    "                        state = random.randint(0, len(states)-1)\n",
    "                episode['s'][t+1] = state\n",
    "                reward = states[state]['reward']\n",
    "                episode['r'][t+1] = reward\n",
    "\n",
    "                if states[state]['terminal']: \n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    # Select and store a_t+1\n",
    "                    action = epsilon_greedy_policy(q, state)\n",
    "                    episode['a'][t+1] = action\n",
    "\n",
    "            tau = t - n + 1\n",
    "            if tau >= 0:\n",
    "                if t + 1 >= T:\n",
    "                    G = episode['r'][T]\n",
    "                else:\n",
    "                    softmax_probs = softmax_actions(q[episode['s'][t+1]])\n",
    "                    softmax_G = 0\n",
    "                    for a in softmax_probs.keys():\n",
    "                        softmax_G += softmax_probs[a] * q[episode['s'][t+1]][a]\n",
    "                    softmax_G *= d\n",
    "                    G = episode['r'][t+1] + (d * softmax_G)\n",
    "\n",
    "                for k in range(min(t, T - 1), (tau + 1) - 1, -1):\n",
    "                    softmax_G_tree = 0\n",
    "                    softmax_probs_tree = softmax_actions(q[episode['s'][k]])\n",
    "                    for a in softmax_probs_tree.keys():\n",
    "                        if a != episode['a'][k]:\n",
    "                            softmax_G_tree += softmax_probs_tree[a] * q[episode['s'][k]][a]\n",
    "                    G = episode['r'][k] + (d * softmax_G_tree) + (d * softmax_probs_tree[episode['a'][k]] * G)\n",
    "\n",
    "                q[episode['s'][tau]][episode['a'][tau]] += lr * (G - q[episode['s'][tau]][episode['a'][tau]])\n",
    "\n",
    "            t += 1\n",
    "            \n",
    "            if tau == T - 1:\n",
    "                break\n",
    "        if episode_count >= min_episode_count:\n",
    "            break\n",
    "   \n",
    "    return q\n",
    "\n",
    "q = n_step_Tree_Backup(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t2\t3\t4\t5\t6\t7\t8\t\n",
      "25\t0\t0\t-100\t0\t-100\t0\t0\t25\t\n",
      "*\t<-\t<-\t<-\tT\t->\t->\t->\t*\t\n"
     ]
    }
   ],
   "source": [
    "line1 = ''\n",
    "line2 = ''\n",
    "line3 = ''\n",
    "for a in range(0, len(states)):\n",
    "    line1 += str(a) + '\\t'\n",
    "    line2 += str(states[a]['reward']) + '\\t'\n",
    "    if states[a]['terminal']:\n",
    "        line3 += '*'\n",
    "    else:\n",
    "        max_a = max(q[a], key=q[a].get)\n",
    "        line3 += max_a\n",
    "    line3 += '\\t'  \n",
    "print(line1)\n",
    "print(line2)\n",
    "print(line3.replace('teleport', 'T').replace('left', '<-').replace('right', '->'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes to help digest Unifying Algorithm: n-step Q(o)\n",
    "\n",
    "o_k: Degree of Sampling\n",
    "\n",
    "p_k: Importance Sampling Weight (pi(A_k | S_k) / b(A_k | S_k))\n",
    "\n",
    "The formula below may be hard to comprehend why we have the additive term:\n",
    "\n",
    "G = R_k + y * [o_k * p_k  + (1 - o_k) * π(A_k | S_k)] * (G - Q(S_k, A_k)) + y * V\n",
    "\n",
    "When o_k == 0:\n",
    "G = R_k + y * [pi(A_k | S_k)] * (G - Q(S_k, A_k)) + y * V\n",
    "\n",
    "When o_k == 1:\n",
    "G = R_k + y * [p_k] * (G - Q(S_k, A_k)) + y * V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentzerQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
