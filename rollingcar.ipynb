{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = .003\n",
    "DISCOUNT = .85\n",
    "\n",
    "terrain = \\\n",
    "'''\n",
    "\\                       |\n",
    " \\           ___        |\n",
    "  \\         /   \\       |\n",
    "   \\       /     \\_/wwww|\n",
    "    \\_____/             |\n",
    "'''\n",
    "chars = ['\\\\', '-', '_', '/', 'w']\n",
    "grid = terrain.split('\\n')[1:-1]\n",
    "max_length = max([len(r) for r in grid])\n",
    "states = []\n",
    "for a in range(max_length):\n",
    "    for b in range(len(grid)):\n",
    "        if grid[b][a] != ' ':\n",
    "            states.append(grid[b][a])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "LEARNING_RATE = .003\n",
    "DISCOUNT = .85\n",
    "\n",
    "terrain = \\\n",
    "'''\n",
    "\\                       |\n",
    " \\           ___        |\n",
    "  \\         /   \\       |\n",
    "   \\       /     \\_/wwww|\n",
    "    \\_____/             |\n",
    "'''\n",
    "chars = ['\\\\', '-', '_', '/', 'w']\n",
    "grid = terrain.split('\\n')[1:-1]\n",
    "max_length = max([len(r) for r in grid])\n",
    "states = []\n",
    "for a in range(max_length):\n",
    "    for b in range(len(grid)):\n",
    "        if grid[b][a] != ' ':\n",
    "            states.append(grid[b][a])\n",
    "            break\n",
    "\n",
    "def phi(states, state, momentum, action, normalization_factor=10):\n",
    "\n",
    "    phi_vector = []\n",
    "\n",
    "    phi_vector.append(momentum/normalization_factor)\n",
    "\n",
    "    # Action (One Hot Encoding for action)\n",
    "    if action == 'acc':\n",
    "        phi_vector.extend([0, 0, 1])\n",
    "    elif action == 'brake':\n",
    "        phi_vector.extend([1, 0, 0])\n",
    "    else:\n",
    "        phi_vector.extend([0, 1, 0])\n",
    "\n",
    "    # Distance to Goal\n",
    "    w_states = [i for i, s in enumerate(states) if s == 'w']\n",
    "    phi_vector.append((w_states[0] - state)/normalization_factor)\n",
    "\n",
    "    # Number of Slopes Types\n",
    "    num_positive_slopes = 0\n",
    "    num_negative_slopes = 0\n",
    "    num_neutral_slopes = 0\n",
    "    for i in range(state, w_states[0] + 1):\n",
    "        if states[i] == '\\\\':\n",
    "            num_positive_slopes += 1\n",
    "        elif states[i] == '/':\n",
    "            num_negative_slopes += 1\n",
    "        else:\n",
    "            num_neutral_slopes += 1\n",
    "    phi_vector.extend([num_positive_slopes/normalization_factor, num_negative_slopes/normalization_factor, num_neutral_slopes/normalization_factor])\n",
    "\n",
    "    # Slope Types Vs. Momentum\n",
    "    if momentum > num_positive_slopes - num_negative_slopes:\n",
    "        phi_vector.extend([0, 0, 1])\n",
    "    elif momentum < num_positive_slopes - num_negative_slopes:\n",
    "        phi_vector.extend([1, 0, 0])\n",
    "    else:\n",
    "        phi_vector.extend([0, 1, 0])\n",
    "    phi_vector.append((momentum - (num_positive_slopes - num_negative_slopes)) / normalization_factor)\n",
    "    \n",
    "    # Tile encoding\n",
    "    tile_encoding = [0] * 20\n",
    "    index = round((state/len(states)) * (len(tile_encoding) - 1))\n",
    "    value = (num_positive_slopes - num_negative_slopes)/normalization_factor\n",
    "    tile_encoding[index] = value\n",
    "    if index > 0:\n",
    "        tile_encoding[index - 1] = value / 2\n",
    "    if index < len(tile_encoding) - 1:\n",
    "        tile_encoding[index + 1] = value / 2\n",
    "    phi_vector.extend(tile_encoding)\n",
    "\n",
    "    return np.array(phi_vector)\n",
    "\n",
    "\n",
    "action_momentum_dict = {\n",
    "    'acc': 1,\n",
    "    'brake': -1,\n",
    "    'neutral': 0\n",
    "}\n",
    "state_momentum_dict = {\n",
    "    '\\\\': 1,\n",
    "    '/': -1,\n",
    "    '_': 0,\n",
    "    'w': 0\n",
    "}\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(q_weights, states, state, momentum, epsilon=0.3):\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(['acc', 'brake', 'neutral'])\n",
    "    else:\n",
    "        best_q = float('-inf')\n",
    "        best_action = []\n",
    "        for a in ['acc', 'brake', 'neutral']:\n",
    "            q_val = np.dot(q_weights, phi(states, state, momentum, a))\n",
    "            if best_q < q_val:\n",
    "                best_q = q_val\n",
    "                best_action = [a]\n",
    "            elif best_q == q_val:\n",
    "                best_action.append(a)\n",
    "        try:\n",
    "            return random.choice(best_action)\n",
    "        except:\n",
    "            ...\n",
    "\n",
    "\n",
    "def semi_gradient_SARSA(states, lr=LEARNING_RATE, d=DISCOUNT, episode_count=2000, e = .3):\n",
    "\n",
    "    weights_len = len(phi(states, 0, 0, 0))\n",
    "    q_weights = np.random.uniform(-1, 1, size=weights_len)\n",
    "\n",
    "    decay_factor = 0.995  # Decay rate for epsilon, can be tuned\n",
    "    min_epsilon = 0.01\n",
    "\n",
    "    while episode_count > 0:\n",
    "        episode_count -= 1\n",
    "\n",
    "        print(f'Episodes Remaining: {episode_count}', end='\\t')\n",
    "\n",
    "        e = max(min_epsilon, e * decay_factor)\n",
    "\n",
    "        s = 0\n",
    "        m = 0\n",
    "        a = epsilon_greedy_policy(q_weights, states, s, m)\n",
    "        r = 0\n",
    "    \n",
    "        while True:\n",
    "\n",
    "            # Take action A, observe R, S'\n",
    "            action_m = action_momentum_dict[a]\n",
    "            if states[s] == '/':\n",
    "                action_m = min(0, action_m) # Car is too weak to get momentum uphill\n",
    "            m_prime = m + action_m + state_momentum_dict[states[s]]\n",
    "            if m_prime > 0:\n",
    "                s_prime = s + 1\n",
    "            else:\n",
    "                s_prime = s\n",
    "\n",
    "            fin_ep = False\n",
    "            # Terminal state: Enter goal states 'w' fully stopping with momentum (m) as 0.\n",
    "            if states[s_prime] == 'w' and m_prime == 0:\n",
    "                print('Success')\n",
    "                r = 5\n",
    "                fin_ep = True\n",
    "            elif states[s_prime] == '|' or m_prime < 0: # Agent overshot the terrain we provided or ran out of momentum prematurely\n",
    "                print('Failure')\n",
    "                r = -5\n",
    "                fin_ep = True\n",
    "            else:\n",
    "                r = -1 * (m + 1) # Reward slower and softer movement\n",
    "\n",
    "            # Simple SARSA implementation\n",
    "            if fin_ep:\n",
    "                q_weights += lr * (\n",
    "                    r - np.dot(q_weights, phi(states, s, m, a))\n",
    "                ) * phi(states, s, m, a)\n",
    "                break\n",
    "            else:\n",
    "                a_prime = epsilon_greedy_policy(q_weights, states, s_prime, m_prime)\n",
    "                q_weights += lr * (\n",
    "                    r + (d * np.dot(q_weights, phi(states, s_prime, m_prime, a_prime))) - np.dot(q_weights, phi(states, s, m, a))\n",
    "                )\n",
    "\n",
    "                s = s_prime\n",
    "                m = m_prime\n",
    "                a = a_prime\n",
    "\n",
    "semi_gradient_SARSA(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(states, state, momentum, action, normalization_factor=10):\n",
    "\n",
    "    phi_vector = []\n",
    "\n",
    "    phi_vector.append(momentum/normalization_factor)\n",
    "\n",
    "    # Action (One Hot Encoding for action)\n",
    "    if action == 'acc':\n",
    "        phi_vector.extend([0, 0, 1])\n",
    "    elif action == 'brake':\n",
    "        phi_vector.extend([1, 0, 0])\n",
    "    else:\n",
    "        phi_vector.extend([0, 1, 0])\n",
    "\n",
    "    # Distance to Goal\n",
    "    w_states = [i for i, s in enumerate(states) if s == 'w']\n",
    "    phi_vector.append((w_states[0] - state)/normalization_factor)\n",
    "\n",
    "    # Number of Slopes Types\n",
    "    num_positive_slopes = 0\n",
    "    num_negative_slopes = 0\n",
    "    num_neutral_slopes = 0\n",
    "    for i in range(state, w_states[0] + 1):\n",
    "        if states[i] == '\\\\':\n",
    "            num_positive_slopes += 1\n",
    "        elif states[i] == '/':\n",
    "            num_negative_slopes += 1\n",
    "        else:\n",
    "            num_neutral_slopes += 1\n",
    "    phi_vector.extend([num_positive_slopes/normalization_factor, num_negative_slopes/normalization_factor, num_neutral_slopes/normalization_factor])\n",
    "\n",
    "    # Slope Types Vs. Momentum\n",
    "    if momentum > num_positive_slopes - num_negative_slopes:\n",
    "        phi_vector.extend([0, 0, 1])\n",
    "    elif momentum < num_positive_slopes - num_negative_slopes:\n",
    "        phi_vector.extend([1, 0, 0])\n",
    "    else:\n",
    "        phi_vector.extend([0, 1, 0])\n",
    "    phi_vector.append((momentum - (num_positive_slopes - num_negative_slopes)) / normalization_factor)\n",
    "    \n",
    "    # Tile encoding\n",
    "    tile_encoding = [0] * 20\n",
    "    index = round((state/len(states)) * (len(tile_encoding) - 1))\n",
    "    value = (num_positive_slopes - num_negative_slopes)/normalization_factor\n",
    "    tile_encoding[index] = value\n",
    "    if index > 0:\n",
    "        tile_encoding[index - 1] = value / 2\n",
    "    if index < len(tile_encoding) - 1:\n",
    "        tile_encoding[index + 1] = value / 2\n",
    "    phi_vector.extend(tile_encoding)\n",
    "\n",
    "    return np.array(phi_vector)\n",
    "\n",
    "\n",
    "action_momentum_dict = {\n",
    "    'acc': 1,\n",
    "    'brake': -1,\n",
    "    'neutral': 0\n",
    "}\n",
    "state_momentum_dict = {\n",
    "    '\\\\': 1,\n",
    "    '/': -1,\n",
    "    '_': 0,\n",
    "    'w': 0\n",
    "}\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(q_weights, states, state, momentum, epsilon=0.3):\n",
    "\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(['acc', 'brake', 'neutral'])\n",
    "    else:\n",
    "        best_q = float('-inf')\n",
    "        best_action = []\n",
    "        for a in ['acc', 'brake', 'neutral']:\n",
    "            q_val = np.dot(q_weights, phi(states, state, momentum, a))\n",
    "            if best_q < q_val:\n",
    "                best_q = q_val\n",
    "                best_action = [a]\n",
    "            elif best_q == q_val:\n",
    "                best_action.append(a)\n",
    "        try:\n",
    "            return random.choice(best_action)\n",
    "        except:\n",
    "            ...\n",
    "\n",
    "\n",
    "def semi_gradient_SARSA(states, lr=LEARNING_RATE, d=DISCOUNT, episode_count=1000, e = .3):\n",
    "\n",
    "    weights_len = len(phi(states, 0, 0, 0))\n",
    "    q_weights = np.random.uniform(-1, 1, size=weights_len)\n",
    "\n",
    "    decay_factor = 0.995  # Decay rate for epsilon, can be tuned\n",
    "    min_epsilon = 0.01\n",
    "\n",
    "    while episode_count > 0:\n",
    "        episode_count -= 1\n",
    "\n",
    "        print(f'Episodes Remaining: {episode_count}', end='\\t')\n",
    "\n",
    "        e = max(min_epsilon, e * decay_factor)\n",
    "\n",
    "        s = 0\n",
    "        m = 0\n",
    "        a = epsilon_greedy_policy(q_weights, states, s, m)\n",
    "        r = 0\n",
    "    \n",
    "        while True:\n",
    "\n",
    "            # Take action A, observe R, S'\n",
    "            action_m = action_momentum_dict[a]\n",
    "            if states[s] == '/':\n",
    "                action_m = min(0, action_m) # Car is too weak to get momentum uphill\n",
    "            m_prime = m + action_m + state_momentum_dict[states[s]]\n",
    "            if m_prime > 0:\n",
    "                s_prime = s + 1\n",
    "            else:\n",
    "                s_prime = s\n",
    "\n",
    "            fin_ep = False\n",
    "            # Terminal state: Enter goal states 'w' fully stopping with momentum (m) as 0.\n",
    "            if states[s_prime] == 'w' and m_prime == 0:\n",
    "                print('Success')\n",
    "                r = 5\n",
    "                fin_ep = True\n",
    "            elif states[s_prime] == '|' or m_prime < 0: # Agent overshot the terrain we provided or ran out of momentum prematurely\n",
    "                print('Failure')\n",
    "                r = -5\n",
    "                fin_ep = True\n",
    "            else:\n",
    "                r = -1 * (m + 1) # Reward slower and softer movement\n",
    "\n",
    "            # Simple SARSA implementation\n",
    "            if fin_ep:\n",
    "                q_weights += lr * (\n",
    "                    r - np.dot(q_weights, phi(states, s, m, a))\n",
    "                ) * phi(states, s, m, a)\n",
    "                break\n",
    "            else:\n",
    "                a_prime = epsilon_greedy_policy(q_weights, states, s_prime, m_prime)\n",
    "                q_weights += lr * (\n",
    "                    r + (d * np.dot(q_weights, phi(states, s_prime, m_prime, a_prime))) - np.dot(q_weights, phi(states, s, m, a))\n",
    "                )\n",
    "\n",
    "                s = s_prime\n",
    "                m = m_prime\n",
    "                a = a_prime\n",
    "\n",
    "semi_gradient_SARSA(states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentzerQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
